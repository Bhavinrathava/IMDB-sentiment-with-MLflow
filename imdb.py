# -*- coding: utf-8 -*-
"""IMDB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aRz1T4XE3gBy1z-r-jnAE1n7l_XiTQgV
"""

#!python3 -m pip install tensorflow transformers pymongo bs4

import tensorflow as tf
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import transformers
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
from textProcessing import denoise_text
from pymongoGetDb import MongoDBUtility
import mlflow
import sys

os.environ["MLFLOW_TRACKING_URI"] = "https://mlflow-nhyltjfyta-uc.a.run.app"
os.environ["MLFLOW_TRACKING_USERNAME"] = "mlftrackinguser"
os.environ["MLFLOW_TRACKING_PASSWORD"] = "mlftrackingpassword"
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "C:/Users/Bhavin/Downloads/imdbsentan/certs/credentials.json"

mlflow.set_tracking_uri("https://mlflow-nhyltjfyta-uc.a.run.app")
experiment_id = mlflow.get_experiment_by_name("IMDB Analysis experiment")
if experiment_id is None:
    experiment_id = mlflow.create_experiment("IMDB Analysis experiment")
else:
    experiment_id = experiment_id.experiment_id

mlflow.set_experiment(experiment_id)
mlflow.start_run(run_name="Run_LSTM_DENSE")
mlflow.tensorflow.autolog(every_n_iter=2)

def set_seed(seed=42):
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
#     os.environ['TF_DETERMINISTIC_OPS'] = '1'
set_seed(42)

LEARNINGRATE = float(sys.argv[1]) if len(sys.argv) > 1 else 0.001
BATCH_SIZE = int(sys.argv[2]) if len(sys.argv) > 2 else 32
LSTMUNITS = int(sys.argv[3]) if len(sys.argv) > 3 else 32
EPOCHS = int(sys.argv[4]) if len(sys.argv) > 4 else 2

mongoHelper = MongoDBUtility()
coll = mongoHelper.getCollection( dbName = "IMDBSentiment", collectionName = "sentimentData")

df = pd.DataFrame(list(coll.find({})))

#Apply function on review column
df['review']=df['review'].apply(denoise_text)

lb = LabelBinarizer()

df['sentiment'] = lb.fit_transform(df['sentiment'])
df['sentiment']
print(df.head(5))

train_df, test_df = train_test_split(df, test_size = 0.2)
allCols = list(df.columns.values)
X_Cols = ['review']
Y_Cols = ['sentiment']
print("Input Columns : ", X_Cols)
print("Target Columns : ", Y_Cols)




MAX_LENGTH = 1024
model_name = "bert-base-uncased"
class BertDataGenerator(tf.keras.utils.Sequence):
    def __init__(
        self,
        full_texts,
        labels=None,
        batch_size=BATCH_SIZE,
        shuffle=True,
        include_targets=True,
    ):
        self.full_texts = full_texts
        self.labels = labels
        self.shuffle = shuffle
        self.batch_size = batch_size
        self.include_targets = include_targets
        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, do_lower_case = True)
        self.indexes = np.arange(len(self.full_texts))
        self.reshuffle()

    def __len__(self):
        return len(self.full_texts) // self.batch_size

    def __getitem__(self, idx):
        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]
        batch_texts = self.full_texts[indexes]

        encoded = self.tokenizer.batch_encode_plus(
            batch_texts.tolist(),
            add_special_tokens=True,
            max_length=MAX_LENGTH,
            return_attention_mask=True,
            return_token_type_ids=True,
            return_tensors="tf",
            truncation=True,
            padding='max_length'
        )

        input_ids = np.array(encoded["input_ids"], dtype="int32")
        attention_masks = np.array(encoded["attention_mask"], dtype="int32")
        token_type_ids = np.array(encoded["token_type_ids"], dtype="int32")

        if self.include_targets:
            labels = np.array(self.labels[indexes], dtype="float32")
            return [input_ids, attention_masks, token_type_ids], labels
        else:
            return [input_ids, attention_masks, token_type_ids]

    def reshuffle(self):
        if self.shuffle:
            np.random.RandomState(42).shuffle(self.indexes)
            

train_data = BertDataGenerator(
    train_df["review"].values.astype("str"),
    np.array(train_df[Y_Cols]),
    batch_size=BATCH_SIZE,
    shuffle=True,
)
valid_data = BertDataGenerator(
    test_df["review"].values.astype("str"),
    np.array(test_df[Y_Cols]),
    batch_size=BATCH_SIZE,
    shuffle=False,
)

def get_model():
    input_ids = tf.keras.layers.Input(
        shape=(MAX_LENGTH,), dtype=tf.float32, name="input_ids"
    )
    
    attention_masks = tf.keras.layers.Input(
        shape=(MAX_LENGTH,), dtype=tf.float32, name="attention_masks"
    )
    
    token_type_ids = tf.keras.layers.Input(
        shape=(MAX_LENGTH,), dtype=tf.float32, name="token_type_ids"
    )

    ilayer = tf.stack([input_ids, attention_masks, token_type_ids], axis = -1)
    print(ilayer.shape)
    
    lstmlayer = tf.keras.layers.LSTM(LSTMUNITS)
    lstmoutput = lstmlayer(ilayer)
    #denselayer0 = tf.keras.layers.Dense(32, activation = 'relu')(lstmoutput)
    #denselayer = tf.keras.layers.Dense(16, activation = 'relu')(denselayer0) 
    output = tf.keras.layers.Dense(1, activation = 'linear')(lstmoutput)

    #premodel = TFAutoModel.from_pretrained(model_name)
    #premodel.trainable = False
    #preoutput = premodel(input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids)
    #cls_output = preoutput.last_hidden_state
    
    #cls_output = tf.keras.backend.expand_dims(cls_output,axis=-1)
    #flattened = tf.keras.layers.Flatten()(cls_output)
    #output = tf.keras.layers.Dense(1, activation = 'linear')(flattened)
    
    
    model = tf.keras.Model(inputs=[input_ids, attention_masks, token_type_ids], outputs=output)
    model.compile(optimizer=tf.optimizers.Adam(learning_rate=LEARNINGRATE),
                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True,label_smoothing=0.0,axis=-1,name='binary_crossentropy'),
                 metrics=[tf.keras.metrics.RootMeanSquaredError()],
                 )
    return model



tf.keras.backend.clear_session()
model = get_model()
model.summary()
callbacks = [tf.keras.callbacks.ModelCheckpoint("./model-finetuning",monitor='val_loss',save_best_only=True,mode = 'min', verbose = 1)]

model.fit(train_data,validation_data=valid_data,epochs= EPOCHS, callbacks=callbacks)



print("Evaluate on test data")
results = model.evaluate(valid_data)
print("test loss, test acc:", results)

mlflow.end_run()
